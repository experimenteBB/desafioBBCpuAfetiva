{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Desafio BB\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import zipfile\n",
    "\n",
    "if(not os.path.exists('./set_train') ):\n",
    "    print('Descompactando dataset')\n",
    "    with zipfile.ZipFile('./dataset/set_train.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "    print('Dataset descompactado!')\n",
    "\n",
    "caminho_resultado = Path('./resultado')\n",
    "caminho_resultado.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho_imagens = \"./set_train/\"\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "pastas = [f for f in listdir(caminho_imagens) if not isfile(join(caminho_imagens, f))]\n",
    "#pastas\n",
    "\n",
    "#\n",
    "# 0 neutra, 1 feliz, 2 triste, 3 surpreso, 4 bravo\n",
    "# \n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array que armazena o caminho completo para todos os arquivos\n",
    "arquivos = []\n",
    "# sentimentos = ['neutro','feliz','triste','surpreso','bravo']\n",
    "sentimentos_lista = [0,1,2,3,4]\n",
    "qnt_amostra = 0\n",
    "sentimentos=[]\n",
    "\n",
    "for pasta in pastas:\n",
    "    caminho_completo = caminho_imagens+pasta+'/bmp/'\n",
    "    pos_arq_atual = 0\n",
    "    \n",
    "    for f in listdir(caminho_completo):\n",
    "        \n",
    "        arquivos.append([caminho_completo+f])\n",
    "        sentimentos.append(sentimentos_lista[pos_arq_atual])\n",
    "        pos_arq_atual=pos_arq_atual+1\n",
    "   \n",
    "    \n",
    "    pos_arq_atual = 0\n",
    "    qnt_amostra = qnt_amostra+1\n",
    "            \n",
    "if len(arquivos) > 0:\n",
    "    print('Encontrados: '+str(len(arquivos))+' arquivos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "import dlib\n",
    "import cv2\n",
    " \n",
    "print('Detectando os pontos da face de todas as imagens')\n",
    "# p = modelo pré-treinado para reconhecer 68 pontos da face\n",
    "p = \"./lib/shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(p)\n",
    "imagens=[]\n",
    "\n",
    "for x in range(0,len(arquivos)):\n",
    "#     print (x)\n",
    "    image = cv2.imread(arquivos[x][0])\n",
    "    # image = cv2.imread('time.jpg')\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    # Make the prediction and transfom it to numpy array\n",
    "    shape = predictor(gray, rects[0])\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "    \n",
    "    shape_transformado = shape.reshape(-1)\n",
    "    imagens.append(shape_transformado)\n",
    "    \n",
    "\n",
    "print('Detecção concluida')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria conjunto de treino e testes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print('Separando amostra de treino e teste')\n",
    "\n",
    "#Caso queira testar com outras amostras, basta alterar o random state\n",
    "imagens_treino, imagens_teste, y_treino, y_teste = train_test_split(imagens, sentimentos, test_size=0.25, random_state=100)\n",
    "print('Separação concluida com '+ str(len(y_treino))+' imagens de treino e '+str(len(y_teste))+' imagens de teste')\n",
    "\n",
    "# Trecho usado para verificar o tamanho das amostras de treino e teste, com relação aos sentimentos\n",
    "# Sim, tem formas mais eficientes de percorrer os arrays e selecionar, mas ainda estou me acostumando\n",
    "# ao Pythom e tenho Prova de R esta semana =)\n",
    "\n",
    "y_treino\n",
    "neutro=0\n",
    "feliz=0\n",
    "triste=0\n",
    "surpreso=0\n",
    "bravo=0\n",
    "\n",
    "for item in y_treino:\n",
    "    if item==0 :\n",
    "        neutro=neutro+1\n",
    "    if item==1 :\n",
    "        feliz=feliz+1\n",
    "    if item==2 :\n",
    "        triste=triste+1\n",
    "    if item==3 :\n",
    "        surpreso=surpreso+1\n",
    "    if item==4 :\n",
    "        bravo=bravo+1\n",
    "\n",
    "print('Amostra treino:'+str(neutro))\n",
    "print('Neutro:'+str(neutro))\n",
    "print('Feliz:'+str(feliz))\n",
    "print('Triste:'+str(triste))\n",
    "print('Surpreso:'+str(surpreso))\n",
    "print('Bravo:'+str(feliz))\n",
    "\n",
    "y_treino\n",
    "neutro=0\n",
    "feliz=0\n",
    "triste=0\n",
    "surpreso=0\n",
    "bravo=0\n",
    "\n",
    "for item in y_teste:\n",
    "    if item==0 :\n",
    "        neutro=neutro+1\n",
    "    if item==1 :\n",
    "        feliz=feliz+1\n",
    "    if item==2 :\n",
    "        triste=triste+1\n",
    "    if item==3 :\n",
    "        surpreso=surpreso+1\n",
    "    if item==4 :\n",
    "        bravo=bravo+1\n",
    "print()\n",
    "print()\n",
    "print('Amostra teste:'+str(neutro))\n",
    "print('Neutro:'+str(neutro))\n",
    "print('Feliz:'+str(feliz))\n",
    "print('Triste:'+str(triste))\n",
    "print('Surpreso:'+str(surpreso))\n",
    "print('Bravo:'+str(feliz))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Grid Search\n",
    "# O Grid Search ajuda a encontrar hiperparametros de ajuste da CNN, com base\n",
    "# em um conjunto pré-determinado de variações\n",
    "# mostrando ao final quais ajustes produziram a melhor acuracidade do modelo\n",
    "# \n",
    "# Obs: também funciona para outros tipos de modelos\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(imagens_treino)\n",
    "\n",
    "imagens_treino_pad = sc.transform(imagens_treino)\n",
    "imagens_teste_pad = sc.transform(imagens_teste)\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "        {\n",
    "            'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            'solver' : ['lbfgs', 'sgd', 'adam'],\n",
    "            'hidden_layer_sizes': [\n",
    "             (1,),(2,),(3,),(4,),(5,),(6,),(7,),(8,),(9,),(10,),(11,), (12,),(13,),(14,),(15,),(16,),(17,),(18,),(19,),(20,),(21,)\n",
    "             ]\n",
    "        }\n",
    "       ]\n",
    "\n",
    "clf = GridSearchCV(MLPClassifier(), param_grid, cv=3,\n",
    "                           scoring='accuracy')\n",
    "clf.fit(imagens_treino_pad,y_treino)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "#Salva modelo e preparador de dados\n",
    "dump(clf,'./resultado/MLPGridSearch.joblib')\n",
    "dump(sc,'./resultado/sc_new.joblib')\n",
    "\n",
    "y_saida = clf.predict(imagens_teste_pad)\n",
    "print('Acuracia: %.2f' % accuracy_score(y_teste, y_saida))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Melhores parametros:\")\n",
    "print(clf.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
